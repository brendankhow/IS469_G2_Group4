from services.supabase_client import supabase
from typing import List, Dict, Optional

class VectorStore:
    @staticmethod
    def store_resume_embedding(
        student_id: str,
        resume_text: str,
        embedding: List[float],
        filename: str,
        metadata: Optional[Dict] = None
    ) -> Dict:
        """Store resume embedding in pgvector"""
        data = {
            "student_id": student_id,
            "resume_text": resume_text,
            "embedding": embedding,
            "filename": filename,
            "metadata": metadata or {}
        }
        
        response = supabase.table("resume_embeddings").insert(data).execute()
        return response.data[0]
    
    @staticmethod
    def search_similar_resumes(
        query_embedding: List[float],
        top_k: int = 10,
        threshold: float = 0.0
    ) -> List[Dict]:
        """ 
            Search for similar resumes using the match_resumes function
            Runs a vector similarity search using cosine similarity - match_resumes is a function defined in supabase
        """
        response = supabase.rpc(
            "match_resumes",
            {
                "query_embedding": query_embedding,
                "match_count": top_k,
                "match_threshold": threshold
            }
        ).execute()
        
        return response.data
    
    @staticmethod
    def get_resume_by_student_id(student_id: str) -> Optional[Dict]:
        """Get resume for a specific student"""
        response = supabase.table("resume_embeddings")\
            .select("*")\
            .eq("student_id", student_id)\
            .execute()
        return response.data[0] if response.data else None
    
    @staticmethod
    def update_resume_embedding(
        student_id: str,
        resume_text: str,
        embedding: List[float],
        filename: str,
        metadata: Optional[Dict] = None
    ) -> Dict:
        """Update existing resume embedding"""
        data = {
            "resume_text": resume_text,
            "embedding": embedding,
            "filename": filename,
            "metadata": metadata or {},
            "updated_at": "now()"
        }
        
        response = supabase.table("resume_embeddings")\
            .update(data)\
            .eq("student_id", student_id)\
            .execute()
        return response.data[0]
    
    # ========== GitHub Portfolio Methods ==========
    
    @staticmethod
    def store_github_document(
        document_id: str,
        student_id: str,
        repo_name: str,
        text: str,
        embedding: List[float],
        metadata: Dict
    ) -> Dict:
        """
        Store a GitHub repository document (overview or README chunk) in pgvector.
        
        Args:
            document_id: Unique document ID (generated by GitHubDocumentProcessor)
            student_id: Student ID (UUID) who owns this portfolio
            repo_name: Repository name
            text: Document text content
            embedding: Vector embedding
            metadata: Rich metadata including repo_name, type, language, topics, etc.
            
        Returns:
            Inserted document data
        """
        data = {
            "document_id": document_id,
            "student_id": student_id,
            "repo_name": repo_name,
            "text": text,
            "embedding": embedding,
            "metadata": metadata
        }
        
        response = supabase.table("github_embeddings").insert(data).execute()
        return response.data[0]
    
    @staticmethod
    def store_github_documents_batch(
        documents: List[Dict],
        student_id: str
    ) -> List[Dict]:
        """
        Store multiple GitHub documents in batch.
        
        Args:
            documents: List of documents from GitHubDocumentProcessor
                      (each has 'id', 'text', 'embedding', 'metadata')
            student_id: Student ID (UUID) who owns this portfolio
            
        Returns:
            List of inserted documents
        """
        data_batch = [
            {
                "document_id": doc["id"],
                "student_id": student_id,
                "repo_name": doc["metadata"].get("repo_name", ""),
                "text": doc["text"],
                "embedding": doc["embedding"],
                "metadata": doc["metadata"]
            }
            for doc in documents
        ]
        
        response = supabase.table("github_embeddings").insert(data_batch).execute()
        print(f"Stored {len(response.data)} GitHub documents for student {student_id}")
        return response.data
    
    @staticmethod
    def search_github_repos(
        query_embedding: List[float],
        student_id: str,
        top_k: int = 10,
        threshold: float = 0.5,
        language: Optional[str] = None,
        topics: Optional[List[str]] = None,
        min_stars: Optional[int] = None
    ) -> List[Dict]:
        """
        Search for relevant GitHub repositories using vector similarity.
        
        Args:
            query_embedding: Query vector embedding
            student_id: Student ID (UUID) to search within specific portfolio
            top_k: Number of results to return
            threshold: Minimum similarity threshold (default: 0.5)
            language: Filter by programming language (e.g., "Python")
            topics: Filter by topics (e.g., ["machine-learning", "web"])
            min_stars: Filter by minimum star count
            
        Returns:
            List of matching documents with similarity scores
        """
        # Call the match_github_repos RPC function
        params = {
            "query_embedding": query_embedding,
            "filter_student_id": student_id,
            "match_threshold": threshold,
            "match_count": top_k,
            "filter_language": language,
            "filter_topics": topics,
            "filter_min_stars": min_stars
        }
        
        response = supabase.rpc("match_github_repos", params).execute()
        
        return response.data
    
    @staticmethod
    def get_student_github_repos(student_id: str) -> List[Dict]:
        """
        Get all GitHub repository documents for a specific student.
        
        Args:
            student_id: Student ID (UUID)
            
        Returns:
            List of all GitHub documents for the student
        """
        response = supabase.table("github_embeddings")\
            .select("*")\
            .eq("student_id", student_id)\
            .execute()
        return response.data
    
    @staticmethod
    def delete_student_github_repos(student_id: str) -> int:
        """
        Delete all GitHub repository documents for a student.
        Useful when refreshing/updating portfolio data.
        
        Args:
            student_id: Student ID (UUID)
            
        Returns:
            Number of documents deleted
        """
        response = supabase.table("github_embeddings")\
            .delete()\
            .eq("student_id", student_id)\
            .execute()
        
        count = len(response.data) if response.data else 0
        print(f"Deleted {count} GitHub documents for student {student_id}")
        return count
    
    @staticmethod
    def get_student_portfolio_summary(student_id: str) -> Dict:
        """
        Get portfolio summary for a student using the database function.
        
        Args:
            student_id: Student ID (UUID)
            
        Returns:
            Summary dictionary with stats
        """
        response = supabase.rpc(
            "get_student_portfolio_summary",
            {"filter_student_id": student_id}
        ).execute()
        
        if response.data and len(response.data) > 0:
            return response.data[0]
        return {
            "student_name": None,
            "github_username": None,
            "total_repos": 0,
            "total_resume_chunks": 0,
            "top_languages": [],
            "total_stars": 0
        }
    
    @staticmethod
    def search_unified_portfolio(
        query_embedding: List[float],
        student_id: str,
        top_k: int = 10,
        threshold: float = 0.5,
        source_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        Search across both resume and GitHub portfolio data using the unified RPC function.
        Creates a unified knowledge base view.
        
        Args:
            query_embedding: Query vector embedding
            student_id: Student ID (UUID)
            top_k: Total number of results to return
            threshold: Minimum similarity threshold (default: 0.5)
            source_filter: Filter by source - 'resume', 'github', or None for both
            
        Returns:
            List of results with 'source' field indicating origin ('resume' or 'github')
        """
        response = supabase.rpc(
            "match_student_portfolio",
            {
                "query_embedding": query_embedding,
                "filter_student_id": student_id,
                "match_threshold": threshold,
                "match_count": top_k,
                "source_filter": source_filter
            }
        ).execute()
        
        return response.data
    
    @staticmethod
    def search_unified_portfolio_grouped(
        query_embedding: List[float],
        student_id: str,
        top_k: int = 10,
        threshold: float = 0.5
    ) -> Dict[str, List[Dict]]:
        """
        Search across both resume and GitHub portfolio data, grouped by source.
        Alternative to search_unified_portfolio that returns grouped results.
        
        Args:
            query_embedding: Query vector embedding
            student_id: Student ID (UUID)
            top_k: Number of results per source
            threshold: Minimum similarity threshold
            
        Returns:
            Dictionary with 'resume' and 'github' keys containing results
        """
        all_results = VectorStore.search_unified_portfolio(
            query_embedding=query_embedding,
            student_id=student_id,
            top_k=top_k * 2,  # Get more to ensure enough per source
            threshold=threshold
        )
        
        # Group by source
        grouped = {
            "resume": [],
            "github": []
        }
        
        for result in all_results:
            source = result.get("source", "")
            if source in grouped:
                grouped[source].append(result)
        
        # Limit each source to top_k
        grouped["resume"] = grouped["resume"][:top_k]
        grouped["github"] = grouped["github"][:top_k]
        
        return grouped
    @staticmethod
    def search_student_resume(
        student_id: str,
        query_embedding: List[float],
        top_k: int = 5,
        threshold: float = 0.7
    ) -> List[Dict]:
        """
        Performs a semantic search for the most relevant chunks
        within a SINGLE student's resume.
        """
        response = supabase.rpc(
            "match_resumes",
            {
                "query_embedding": query_embedding,
                "match_count": top_k,
                "match_threshold": threshold
            }
        ).eq(
            "student_id", student_id 
        ).execute()
        
        return response.data

    # ========== graphRAG methods ==========

    @staticmethod
    def store_graph_node(
        node_id: str,
        student_id: str,
        text: str,
        embedding: List[float],
        metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Store a single graph node (entity or chunk) with embedding and metadata.
        Each node should have a student_id so recruiters know which candidate it belongs to.
        """
        data = {
            "node_id": node_id,
            "student_id": student_id,
            "text": text,
            "embedding": embedding,
            "metadata": metadata or {}
        }
        response = supabase.table("graphrag_portfolio").insert(data).execute()
        return response.data[0]

    @staticmethod
    def store_graph_nodes_batch(
        nodes: List[Dict]
    ) -> List[Dict]:
        """
        Store multiple graph nodes in a single batch insert.
        Each node dictionary must include 'id', 'text', 'embedding', and optionally 'student_id' and 'metadata'.
        """
        data_batch = [
            {
                "node_id": node["id"],
                "student_id": node.get("student_id", "unknown"),
                "text": node["text"],
                "embedding": node["embedding"],
                "metadata": node.get("metadata", {})
            }
            for node in nodes
        ]
        response = supabase.table("graphrag_portfolio").insert(data_batch).execute()
        print(f"Stored {len(response.data)} GraphRAG nodes in graphrag_portfolio")
        return response.data

    @staticmethod
    def search_graph_nodes(
        query_embedding: List[float],
        top_k: int = 10,
        threshold: float = 0.5,
        filters: Optional[Dict] = None  # optional metadata filters for recruiter queries
    ) -> List[Dict]:
        """
        Search for relevant graph nodes (chunks/entities) globally across all students.
        Optional filters can restrict results by skills, roles, or other metadata.
        """
        params = {
            "query_embedding": query_embedding,
            "match_threshold": threshold,
            "match_count": top_k
        }

        if filters:
            params.update(filters)

        try:
            response = supabase.rpc("match_graph_nodes", params).execute()
            return response.data
        except Exception as e:
            print(f"Warning: match_graph_nodes RPC failed: {e}")
            # Fallback: return empty list if RPC fails
            return []

    @staticmethod
    def delete_graph_nodes(student_id: Optional[str] = None) -> int:
        """
        Delete GraphRAG nodes.
        If student_id is provided, deletes only that student's nodes.
        Otherwise, deletes all nodes (use with caution!).
        """
        query = supabase.table("graphrag_portfolio").delete()
        if student_id:
            query = query.eq("student_id", student_id)
        response = query.execute()
        count = len(response.data) if response.data else 0
        if student_id:
            print(f"Deleted {count} GraphRAG nodes for student {student_id}")
        else:
            print(f"Deleted {count} GraphRAG nodes globally")
        return count

    @staticmethod
    def get_all_candidates_documents() -> List[Dict]:
        """
        Get all candidate documents (resumes and GitHub repos) for GraphRAG indexing.
        Returns documents in a format compatible with GraphRAG processing.
        """
        all_documents = []
        
        # get all resume documents
        resume_response = supabase.table("resume_embeddings")\
            .select("student_id, resume_text, filename")\
            .execute()
        
        for resume in resume_response.data:
            all_documents.append({
                "doc_id": f"resume_{resume['student_id']}",
                "student_id": resume['student_id'],
                "text": resume['resume_text'],
                "source": "resume",
                "filename": resume.get('filename', 'unknown')
            })
        
        # get all github documents
        github_response = supabase.table("github_embeddings")\
            .select("student_id, document_id, text, repo_name, metadata")\
            .execute()
        
        for github_doc in github_response.data:
            all_documents.append({
                "doc_id": github_doc['document_id'],
                "student_id": github_doc['student_id'],
                "text": github_doc['text'],
                "source": "github",
                "repo_name": github_doc.get('repo_name', 'unknown')
            })
        
        print(f"Retrieved {len(all_documents)} total documents for GraphRAG indexing")
        print(f"  - Resumes: {len(resume_response.data)}")
        print(f"  - GitHub docs: {len(github_response.data)}")
        
        return all_documents